# AV-ASR 基础模型开发需求文档
## 一、项目概述
### 1. 项目目标
开发一款**极简架构的视听融合语音识别（AV-ASR）基础模型**，不依赖大模型（参数量控制在100M以内），通过独立的视频特征编码器与音频特征编码器提取双模态特征，最终经CTC解码实现时序对齐与语音转录，验证视觉特征对音频识别的基础互补作用。

### 2. 核心定位
- 技术定位：基础验证型模型，聚焦“双模态编码+CTC解码”的核心流程，不引入复杂模块（如自注意力、多任务训练、预训练微调）；
- 应用场景：适用于干净/低噪声环境的短语音转录（5秒内 utterance），支持英文基础词汇识别（核心验证功能，非生产级应用）；
- 核心价值：快速验证“视听融合”的技术可行性，为后续复杂模型优化提供基础参照。

## 二、功能需求
### 1. 输入输出
- 输入：
  - 音频：16kHz采样率、单通道、WAV格式音频片段（时长1-5秒）；
  - 视频：30fps帧率、128×128分辨率、仅含单人唇部区域的视频片段（与音频时序严格同步）；
- 输出：
  - 转录文本：英文小写字母+空格+基础标点（. , ! ?）；
  - 辅助输出：CTC解码的帧级token概率分布（可选，用于调试）。

### 2. 核心功能
- 双模态特征提取：独立编码音频与视频特征，不涉及跨模态早期融合；
- CTC时序对齐：通过CTC损失解决“音频/视频帧序列→文本token序列”的非对齐问题；
- 基础识别能力：在干净环境下，对500词以内英文词汇的转录WER（词错误率）≤15%；
- 轻量化推理：单条5秒音频+视频推理延迟≤100ms（CPU环境）。

### 3. 非功能需求
- 模型规模：总参数量≤100M，避免复杂网络结构（如Transformer、LSTM深层堆叠）；
- 训练效率：单GPU（16GB显存）训练时长≤24小时，支持批量训练（batch size≥16）；
- 兼容性：支持PyTorch 1.10+框架，可导出ONNX格式用于部署；
- 可调试性：输出训练过程中的WER、损失值曲线，支持单样本特征可视化。

## 三、技术需求
### 1. 数据要求
- 数据集：采用公开小型AV-ASR数据集（如LRW-subset、GRID），包含：
  - 训练集：≥1万条样本（音频+视频+转录文本），覆盖10-20名说话人；
  - 验证集/测试集：各≥1千条样本，与训练集说话人无重叠；
- 数据预处理：
  - 音频：提取40维log-mel谱图特征（25ms窗口、10ms步长），全局归一化；
  - 视频：裁剪唇部区域（128×128），转为灰度图，按帧提取64维HOG特征（或简单CNN特征）；
  - 文本：统一为小写，过滤特殊字符，构建≤500词的词汇表（含空白符<blank>）。

### 2. 模型架构（极简设计）
三、技术需求
1. 数据要求
数据集：采用公开小型 AV-ASR 数据集（如 LRW-subset、GRID），包含：
训练集：≥1 万条样本（音频 + 视频 + 转录文本），覆盖 10-20 名说话人；
验证集 / 测试集：各≥1 千条样本，与训练集说话人无重叠；
数据预处理：
音频：提取 40 维 log-mel 谱图特征（25ms 窗口、10ms 步长），全局归一化；
视频：裁剪唇部区域（128×128），转为灰度图，按帧提取 64 维 HOG 特征（或简单 CNN 特征）；
文本：统一为小写，过滤特殊字符，构建≤500 词的词汇表（含空白符<blank>）。
2. 模型架构（轻量化融合 MLCA）
（1）音频编码器
结构：3 层 2D 卷积网络（Conv2d）+ 2 层全连接层；
参数：卷积核大小 3×3，通道数依次为 32→64→128， stride=2；全连接层输出维度 256；
激活函数：ReLU，无 BatchNorm（简化训练）；
输出：音频时序特征序列（维度：T_a × 256，T_a 为音频帧数量）。
（2）视频编码器
结构：与音频编码器对称，3 层 2D 卷积网络（Conv2d）+ 2 层全连接层；
参数：卷积核大小 3×3，通道数依次为 32→64→128， stride=2；全连接层输出维度 256；
激活函数：ReLU，无 BatchNorm；
输出：视频时序特征序列（维度：T_v × 256，T_v 为视频帧数量，需与 T_a 做时序对齐）。
（3）多尺度跨模态注意力（MLCA）模块（核心替换）
核心目标：替代简单特征拼接，实现音频 / 视频特征的自适应交互与对齐；
轻量化设计约束：参数量≤10M，不引入多头注意力堆叠；
结构细节：
时序对齐层：通过 1×1 卷积将音频 / 视频特征序列映射到相同时序长度（取 T_a、T_v 均值，线性插值补帧 / 降采样）；
多尺度特征映射：分别对音频 / 视频特征做 2 个尺度的 1D 卷积（核大小 3、5），生成多尺度特征（维度：T × 128）；
跨模态注意力计算：
相似度矩阵：计算音频多尺度特征与视频多尺度特征的余弦相似度（维度：T×T）；
注意力权重：通过 softmax 归一化相似度矩阵，分别生成 “音频关注视频”“视频关注音频” 的注意力权重；
特征增强：用注意力权重加权融合对方模态的多尺度特征，再与自身模态特征拼接；
融合输出：将增强后的音频 / 视频特征拼接，输出 512 维融合特征序列（维度：T × 512）；
激活函数：GELU（轻量化，优于 ReLU 的非线性拟合），仅在注意力输出层添加 Dropout（概率 0.1）。
（4）CTC 头
结构：1 层全连接层，将 512 维融合特征映射到词汇表维度（含<blank>，维度 = 501）；
损失函数：标准 CTC 损失（blank_id=0，reduction="mean"）；
优化：添加层归一化（LayerNorm），解决 MLCA 引入的分布偏移问题（不增加过多参数）。


详细说明
整体架构总览
模型采用双模态独立编码 + 多尺度跨模态注意力（MLCA）融合 + CTC 解码 三层架构，整体计算流程：
plaintext
音频波形/视频帧序列 → 模态前端特征提取 → 模态编码器 → 时序对齐 → MLCA多尺度跨模态交互 → 融合特征 → CTC头 → 字符概率分布 → CTC解码 → 转录文本
总参数量严格控制在 100M 以内，各模块参数量占比：音频编码器（25%）+ 视频编码器（25%）+ MLCA 模块（10%）+ CTC 头（40%）。
（1）音频编码器（Audio Encoder）
核心目标：从原始音频波形中提取低 / 中阶声学特征，保留时序信息，输出固定维度的音频特征序列。
表格
层级	类型	核心参数	输入维度	输出维度	激活函数	正则化	参数量（约）
输入层	原始输入	16kHz 单通道 WAV 音频，时长 1-5 秒	(B, 1, L_a)	-	-	-	0
卷积层 1	Conv2d	kernel_size=(3,3), stride=(2,2), padding=(1,1), out_channels=32	(B, 1, F, T_a)	(B, 32, F/2, T_a/2)	ReLU	-	~1.8k
卷积层 2	Conv2d	kernel_size=(3,3), stride=(2,2), padding=(1,1), out_channels=64	(B, 32, F/2, T_a/2)	(B, 64, F/4, T_a/4)	ReLU	-	~18.4k
卷积层 3	Conv2d	kernel_size=(3,3), stride=(2,2), padding=(1,1), out_channels=128	(B, 64, F/4, T_a/4)	(B, 128, F/8, T_a/8)	ReLU	Dropout(0.1)	~73.8k
展平层	Permute+Flatten	permute (0,3,1,2) → flatten (2)，将频域维度合并	(B, 128, F/8, T_a/8)	(B, T_a/8, 128×F/8)	-	-	0
全连接层 1	Linear	in_features=128×F/8（F=40，即 640），out_features=512	(B, T_a/8, 640)	(B, T_a/8, 512)	ReLU	-	~328k
全连接层 2	Linear	in_features=512，out_features=256	(B, T_a/8, 512)	(B, T_a', 256)	ReLU	Dropout(0.1)	~131k
补充说明：
F 为音频 log-mel 谱图的频域维度（固定 40），T_a 为音频帧数量（16kHz 采样、25ms 窗口、10ms 步长时，5 秒音频 T_a=498）；
T_a' = T_a/8（卷积层总下采样倍数为 8），最终输出音频特征序列维度为 (批次 B, 时序长度 T_a', 特征维度 256)；
所有卷积层采用 “same padding” 保证时序维度下采样仅由 stride 控制，无信息丢失；
展平层先交换维度（将时序维度 T_a/8 前置），再展平频域 + 通道维度，保证输出为 “时序 - 特征” 结构。
（2）视频编码器（Video Encoder）
核心目标：从唇部视频帧中提取时空特征，与音频编码器输出维度对齐，保证跨模态交互可行性。
表格
层级	类型	核心参数	输入维度	输出维度	激活函数	正则化	参数量（约）
输入层	原始输入	30fps、128×128 分辨率、单人唇部灰度视频，时长 1-5 秒	(B, 1, T_v, 128, 128)	-	-	-	0
3D 卷积层 1	Conv3d	kernel_size=(3,3,3), stride=(1,2,2), padding=(1,1,1), out_channels=32	(B, 1, T_v, 128, 128)	(B, 32, T_v, 64, 64)	ReLU	-	~8.8k
3D 卷积层 2	Conv3d	kernel_size=(3,3,3), stride=(1,2,2), padding=(1,1,1), out_channels=64	(B, 32, T_v, 64, 64)	(B, 64, T_v, 32, 32)	ReLU	-	~73.8k
3D 卷积层 3	Conv3d	kernel_size=(3,3,3), stride=(1,2,2), padding=(1,1,1), out_channels=128	(B, 64, T_v, 32, 32)	(B, 128, T_v, 16, 16)	ReLU	Dropout(0.1)	~295k
全局平均池化层	AvgPool3d	kernel_size=(1,16,16), stride=1（仅池化空间维度）	(B, 128, T_v, 16, 16)	(B, 128, T_v, 1, 1)	-	-	0
展平层	Flatten	flatten (2)，合并空间维度	(B, 128, T_v, 1, 1)	(B, T_v, 128)	-	-	0
全连接层 1	Linear	in_features=128，out_features=512	(B, T_v, 128)	(B, T_v, 512)	ReLU	-	~66k
全连接层 2	Linear	in_features=512，out_features=256	(B, T_v, 512)	(B, T_v', 256)	ReLU	Dropout(0.1)	~131k
补充说明：
T_v 为视频帧数量（30fps×5 秒 = 150 帧），T_v' = T_v（仅池化空间维度，时序维度保留）；
3D 卷积层 stride=(1,2,2)：仅对空间维度下采样，时序维度保持不变，保证视频时序与音频时序的对应性；
全局平均池化层仅池化空间维度（16×16），输出维度为 (B, 128, T_v, 1, 1)，展平后得到 “时序 - 特征” 结构；
最终输出视频特征序列维度为 (批次 B, 时序长度 T_v', 特征维度 256)，与音频编码器输出特征维度完全对齐。
（3）时序对齐层（Temporal Alignment Layer）
核心目标：解决音频 / 视频时序长度不一致问题（T_a' ≠ T_v'），保证跨模态注意力计算的维度匹配。
表格
操作类型	适用模态	核心逻辑	输入维度	输出维度	参数量
线性插值 / 降采样	音频 / 视频	1. 计算目标时序长度 T = max (T_a', T_v')；
2. 对短序列做线性插值补帧，对长序列做均值降采样；
3. 最终音频 / 视频序列时序长度统一为 T	(B, T_a'/T_v', 256)	(B, T, 256)	0
补充说明：
插值 / 降采样仅作用于时序维度，特征维度（256）保持不变；
采用线性插值而非随机采样，避免破坏时序连续性，保证跨模态特征的时序对应性；
对齐后音频 / 视频特征序列维度均为 (B, T, 256)，为 MLCA 模块提供统一输入。
（4）多尺度跨模态注意力（MLCA）模块（核心）
核心目标：替代简单特征拼接，实现音频 / 视频特征在多尺度下的双向交互与信息互补，轻量化设计保证参数量≤10M。
表格
子模块	操作类型	核心参数	输入维度	输出维度	激活函数	正则化	参数量（约）
多尺度特征映射（音频）	1D 卷积组	conv1d_3: kernel_size=3, stride=1, padding=1, out_channels=128；
conv1d_5: kernel_size=5, stride=1, padding=2, out_channels=128	(B, T, 256)	(B, T, 128) × 2	GELU	-	~197k (×2)
多尺度特征映射（视频）	1D 卷积组	同音频多尺度卷积参数	(B, T, 256)	(B, T, 128) × 2	GELU	-	~197k (×2)
特征拼接（音频）	Concatenate	dim=-1，拼接 conv1d_3+conv1d_5 输出	(B, T, 128) × 2	(B, T, 256)	-	-	0
特征拼接（视频）	Concatenate	dim=-1，拼接 conv1d_3+conv1d_5 输出	(B, T, 128) × 2	(B, T, 256)	-	-	0
相似度矩阵计算	Cosine Similarity	dim=-1，计算音频 / 视频拼接特征的帧级余弦相似度	(B, T, 256) × 2	(B, T, T)	-	-	0
注意力权重归一化	Softmax	dim=-1，对相似度矩阵每行做 softmax	(B, T, T)	(B, T, T)（音频→视频）、(B, T, T)（视频→音频）	-	-	0
音频特征增强	矩阵乘法	音频注意力权重 × 视频拼接特征	(B, T, T) + (B, T, 256)	(B, T, 256)	-	-	0
视频特征增强	矩阵乘法	视频注意力权重 × 音频拼接特征	(B, T, T) + (B, T, 256)	(B, T, 256)	-	-	0
残差连接（音频）	Add	原始音频拼接特征 + 增强音频特征	(B, T, 256) × 2	(B, T, 256)	-	-	0
残差连接（视频）	Add	原始视频拼接特征 + 增强视频特征	(B, T, 256) × 2	(B, T, 256)	-	-	0
融合特征拼接	Concatenate	dim=-1，拼接残差后的音频 + 视频特征	(B, T, 256) × 2	(B, T, 512)	GELU	Dropout(0.1)	0
融合特征投影	Linear	in_features=512，out_features=512	(B, T, 512)	(B, T, 512)	GELU	-	~262k
补充说明：
多尺度特征映射：通过不同核大小的 1D 卷积（3/5）捕捉音频 / 视频特征的局部（短时序）和全局（长时序）信息，提升跨模态交互的覆盖范围；
双向注意力计算：
音频→视频注意力：音频每帧关注视频所有帧的多尺度特征，捕捉 “音频音素 - 视频唇部动作” 的对应关系；
视频→音频注意力：视频每帧关注音频所有帧的多尺度特征，弥补音频噪声导致的特征丢失；
残差连接：保留原始模态特征的核心信息，避免跨模态交互导致的特征稀释，是解决原架构 WER 100% 的关键设计；
最终输出：融合特征序列维度为 (B, T, 512)，参数量总计约 980k（≤10M），满足轻量化要求。
（5）CTC 头（CTC Decoder Head）
核心目标：将融合特征序列映射为字符概率分布，通过 CTC 损失实现时序对齐与识别。
表格
层级	类型	核心参数	输入维度	输出维度	激活函数	正则化	参数量（约）
层归一化层	LayerNorm	normalized_shape=512，eps=1e-5	(B, T, 512)	(B, T, 512)	-	-	~1k
全连接层 1	Linear	in_features=512，out_features=1024	(B, T, 512)	(B, T, 1024)	GELU	Dropout(0.1)	~525k
全连接层 2（输出层）	Linear	in_features=1024，out_features=V（V=501，词汇表大小 +<blank>）	(B, T, 1024)	(B, T, V)	LogSoftmax	-	~513k
补充说明：
词汇表大小 V=500（英文基础词汇 + 标点）+1（<blank>空白符）=501；
层归一化层作用于融合特征序列，解决 MLCA 模块输出特征分布偏移问题，提升 CTC 损失的收敛性；
输出层采用 LogSoftmax 激活，直接适配 PyTorch 的 CTC Loss 计算（无需额外归一化）；
最终输出维度为 (B, T, V)，表示每个时序帧对应每个字符的对数概率。
### 3. 训练配置
- 优化器：Adam，初始学习率1e-4，学习率衰减策略：验证集WER3轮无下降则衰减为原来的0.5；
- 训练轮数：最多50轮，早停策略（验证集WER5轮无下降则停止）；
- 正则化：仅使用Dropout（概率0.1，仅全连接层添加）；
- 批量处理：动态padding（音频/视频帧序列按批次最长长度padding，填充值=0）。

### 4. 解码配置
- 解码方式：基础贪心解码（逐帧取概率最大token）；
- 后处理：按CTC规则折叠路径（合并相邻重复token+删除空白符<blank>）；
- 可选优化：支持束搜索（beam size=5，仅用于测试，不增加训练复杂度）。

## 四、交付物
1. 源代码：
   - 数据预处理脚本（音频/视频特征提取、文本编码）；
   - 模型定义代码（音频编码器、视频编码器、CTC头）；
   - 训练/验证/测试脚本（含损失计算、WER评估、早停逻辑）；
   - 推理脚本（支持单样本/批量推理，输出转录文本）。
2. 模型文件：
   - 训练完成的最优模型权重（.pth格式）；
   - ONNX格式导出文件（用于部署验证）。
3. 文档：
   - 数据预处理说明（含特征提取细节、词汇表）；
   - 模型架构详细参数表（各层输入输出维度、参数量）；
   - 训练日志（损失值、WER变化曲线）；
   - 测试报告（测试集WER、推理延迟、典型样本转录结果）。

## 五、约束条件
1. 技术限制：
   - 禁止使用预训练模型（如CLIP、wav2vec系列）；
   - 禁止使用复杂网络模块（如Transformer、LSTM、注意力机制）；
   - 禁止引入额外模态或多任务训练（仅聚焦“音频+视频→文本”转录）。
2. 环境限制：
   - 训练环境：单GPU（≥16GB显存），CPU≥8核，内存≥32GB；
   - 推理环境：支持CPU/GPU，无特殊硬件依赖。